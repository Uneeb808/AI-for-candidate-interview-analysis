# InsightHire AI

A next-generation web-based interview analysis platform that brings objectivity, consistency and deep behavioral insights to remote interviews.

---

## ğŸ“‹ Problem Statement

In todayâ€™s remote-first world, traditional interviews rely heavily on subjective human judgment:

- **Bias & Inconsistency:** Different interviewers evaluate the same candidate differently.  
- **Missed Non-verbal Cues:** Stress, confidence, engagement and other signals are hard to gauge over video.  
- **Limited Data:** Interviewers canâ€™t easily quantify performance trends or compare candidates objectively.  
- **Remote Challenges:** In digital interactions, emotional and behavioral cues are even harder to assess.

**Who is affected?**  
- **Recruiters & Hiring Managers:** Struggle to consistently rank candidates.  
- **Candidates:** Receive little actionable feedback.  
- **Educators & Therapists:** Canâ€™t scale remote engagement or stress monitoring.

---

## ğŸ›  Our Solution

**InsightHire AI** integrates real-time computer vision, audio analytics, and ML to compute multidimensional behavioral scores, displayed in an interactive dashboard:

1. **Emotion Analysis** via facial-expression recognition + voice pitch & energy metrics.  
2. **Eye Contact Tracking** using MediaPipe FaceMesh.  
3. **Posture & Movement** scoring via MediaPipe Pose.  
4. **Anxiety Detection** through blink count, face touches, jaw tension.  
5. **Expressive Speech**â€”pitch variability, range, energy variance.  
6. **Composite â€œEngagementâ€, â€œConfidenceâ€, â€œStressâ€ & â€œProfessionalismâ€** time-series data and summary averages.  

At the end, an **Overall Score** (0â€“10) combines all four pillars with customizable weights.

---

## ğŸ” Parameter Definitions & Scoring

| **Parameter**        | **How We Measure**                                                            | **Score Calculation**                                                                                           |
|----------------------|--------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| **Emotion**          | 1. Facial expressions (HSEmotionRecognizer)<br>2. Voice pitch & energy metrics  | 1. **Audio Emotion Value** (0â€“1) via prosody/sentiment model<br>2. **Facial Emotion Proxy** (0â€“1) via landmark averages<br>3. **Pitch Variability** (std Hz â†’ normalized 0â€“1)<br>4. **Pitch Range** (Hz range â†’ norm 0â€“1)<br>5. **Energy Variance** (RMS coeff. of var. â†’ norm 0â€“1)<br><br>**Weighted base:** 35% audio + 35% facial + 15% pitch + 15% energy<br>**Adjustments:** penalties for mixed/unnatural signals, bonuses for consistency<br>â†’ **Scaled to 0â€“10** |
| **Eye Contact**      | Gaze offset vs. face-center + EAR threshold for open eyes                      | - Compute average EAR (eye aspect ratio) to ensure eyes open<br>- Measure gaze-center offset normalized to frame width<br>- **Score/frame:** `10 * (1 â€“ offset/threshold)` clamped [0â€“10]<br>- **Final:** `10 * (1 â€“ poor_frames/total_frames)`                      |
| **Posture**          | Shoulder & hip tilt stability                                                   | - Measure frame-to-frame shoulder & hip height difference<br>- Detect shifts > 0.02 normalized units<br>- **Frame shifts ratio** â†’ penalize: `score = max(0, 10 â€“ shift_ratio*15)` |
| **Anxiety**          | Blink rate, face touches, jaw tension, eye avoidance                            | - Count blinks (EAR drops)<br>- Detect face touches (finger â†” nose < 0.05 norm)<br>- Jaw width < threshold â†’ tension<br>- Eye avoidance frames<br>- **Combined ratio:** `(blinks + touches + tension_events + avoidance)/frames` â†’ `anxiety_score = 10*(1 â€“ ratio)` |

### Composite â€œNewâ€ Categories (per-frame â†’ summary)

| Category         | Formula (per frame)                                                                                                 |
|------------------|----------------------------------------------------------------------------------------------------------------------|
| **Engagement**   | 0.6Ã—(eye_contact_frame) + 0.4Ã—(posture_frame)                                                                        |
| **Confidence**   | 0.5Ã—(eye_contact_frame) + 0.4Ã—(pitch_range*10) + 0.1Ã—(posture_frame)                                                 |
| **Stress**       | `10 â€“ (0.6*(10 â€“ anxiety_frame) + 0.4*facial_emotion_frame)`                                                         |
| **Professionalism** | 0.5Ã—(posture_frame) + 0.5Ã—(energy_variance*10)                                                                   |

Each is aggregated across all frames to yield a **final average (0â€“10)**.

---

## ğŸ“Š Final Score & Visualization

- **Weights:** Emotion 40% + Eye Contact 20% + Posture 20% + Anxiety 20%  
- **Interactive Timeline:** Chart.js line graph of Engagement, Confidence, Stress, Professionalism over time.  
- **Dashboard:** Circular progress rings for each pillar, detailed counts, head-nod tally, pitch & energy stats.

---

## Installation Required-

1. **Prerequisites:**  
   ```bash
   pip install -r requirements.txt
   # ensures: Flask, OpenCV, MediaPipe, hsemotion-onnx, librosa, ffmpegâ€¦
